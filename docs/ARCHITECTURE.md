# HalaAI Architecture Flow

This document summarizes how requests move through HalaAI from client to GPU and back.

## High-Level Runtime

```
Client/UI/Tools
    |
    v
FastAPI app (app/main.py)
    |
    +--> HTTP /chat (blocking) -> engine.generate_text
    |
    +--> WebSocket /ws/chat/v2 (streaming)
           -> session tracking + history append (app/session_manager.py)
           -> memory recall (core/memory.py)
           -> summary recall (Chroma)
           -> search/expand intent probe (app/prompts.py)
           -> Brave deep search + browse (core/search/brave_browse.py)
           -> transcript expansion (data/sql/expander.py)
           -> engine.generate_stream (queue + GPU worker)
```

## Connection Diagram (Where `main.py` Fits)

```
run_server.py
    |
    v
app/main.py  ->  FastAPI app instance
    |
    +--> include_router(app/ws_chat.py)  ->  registers /ws/chat/v2
    +--> include_router(data/service/history_api.py) -> /data/sessions + /data/session
    +--> include_router(data/service/vector_api.py)  -> /data/vector/search
    |
    +--> defines HTTP routes (/chat, /adapters/load)
    |
    +--> startup/shutdown hooks (start queue worker + monitor + session sweeper)
```

## Entry Points

- `run_server.py` starts Uvicorn with `app.main:app`.
- `app/main.py` registers routes and starts the engine background tasks on startup.

## WebSocket Streaming Flow (`/ws/chat/v2`)

1) Client sends `session_start` with `session_id`, then messages containing `prompt`, `max_tokens`, and optional `system_prompt` + `priority`.
2) `app/ws_chat.py` emits a `{"type": "status"}` message (e.g., "Thinking...").
3) The session is created/updated in Postgres and the message is appended to `sessions.history`.
4) The API layer recalls:
   - verified user profile (memory recall),
   - related chat summaries (vector DB),
   - and current session history (Postgres).
5) A short "search/expand-intent probe" pass is run to detect `[SEARCH: ...]` or `[EXPAND: ...]`.
6) If search is requested, `core/search/brave_browse.py` performs Brave search + page scraping.
7) If expand is requested, `data/sql/expander.py` fetches the full transcript and injects it as prior dialogue.
8) Search/expansion blocks are formatted by `app/prompts.py` and injected into the system prompt.
9) `engine.generate_stream(request)` enqueues a job in `app/queue.py`.
10) The GPU worker (`ModelEngine._worker_loop` in `app/engine.py`) consumes the job, runs
    `stream_generate`, and pushes tokens onto the response queue.
11) `app/ws_chat.py` streams tokens back as `{"type": "token"}` messages and finishes with
    `{"type": "end"}`; the assistant response is appended to `sessions.history`.
12) Inference stats are logged via `app/database.py`, and hardware metrics come from `app/monitor.py`.

## HTTP Blocking Flow (`/chat`)

- `app/main.py` receives a POST request and calls `engine.generate_text`.
- This path runs inference directly under the engine lock (no queue, no streaming).
- Memory recall, deep search, and session history are not applied here; they are only done in the WebSocket API layer.

## Memory + Session Flow

- Storage: `core/memory.py` uses ChromaDB at `data/vector_db/`.
- Chat sessions: Postgres table `sessions` (see `data/sql/database.py`).
- Summaries: generated by `app/session_manager.py` and saved to both Postgres + Chroma (`source="chat_summary"`).
- Recall: `core/memory.py` embeds the query, pulls top matches, and filters by distance threshold.

## Prompt Assembly

- `app/prompts.py` injects exact local date/time for temporal grounding.
- User profile memories are labelled as verified system records to prioritise them over speculation.
- Expanded transcripts are labelled explicitly as prior dialogue context.

## Queue + GPU Worker

- `app/queue.py` is a priority queue (lower number = higher priority).
- `engine.start_background_tasks()` spins up:
  - a queue worker to run GPU inference, and
  - a queue monitor to log depth + latency.
- `engine.generate_stream` enqueues a job and yields tokens from a per-request response queue.

## Adapters (LoRA)

- `app/engine.py` loads LoRA adapters from `adapters/`.
- `/adapters/load` swaps adapters without reloading the base model.

## UI Flow (Chainlit)

- `ui/app.py` connects to `/ws/chat/v2`, displays status messages, and streams tokens.
- `ui/chainlit.md` documents the UI startup steps.

## Examples

- `examples/ws_chat_cli.py` streams from `/ws/chat/v2`.
- `examples/basic_api.py` uses the blocking HTTP `/chat` endpoint.
- `examples/langchain/HalaLLM.py` wraps the HTTP `/chat` endpoint for LangChain.

## Data + Artifacts

- `data/vector_db/`: ChromaDB persistent store for memory.
- `data/sql/`: Postgres helpers, expander, reset script.
- `data/service/`: FastAPI data endpoints.
- `inference_logs.db`: inference logs via SQLModel.
- `performance/`: performance plots and logs.
- `evals/`: evaluation datasets and reports.
